{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "create database if not exists finance_data_catalog\n",
        "\n",
        "%sql\n",
        "use finance_data_catalog\n",
        "\n",
        "# Creating Transaction Table\n",
        "%sql\n",
        "create table if not exists transaction_data(TransactionID String,CustomerID String, TransactionAmount INT, TransactionDate Date)\n",
        "\n",
        "#creating Customer_table\n",
        "%sql\n",
        "create table if not exists customer_data(CustomerID String,CustomerName String, Email String, Country String)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gv3PRJKDufrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meta data retrieval\n",
        "%sql\n",
        "show tables in finance_data_catalog;\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"FinanceDataCatalog\").getOrCreate()\n",
        "#profiling\n",
        "\n",
        "# Profile transaction amounts\n",
        "spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        MIN(TransactionAmount) AS MinAmount,\n",
        "        MAX(TransactionAmount) AS MaxAmount,\n",
        "        AVG(TransactionAmount) AS AvgAmount,\n",
        "        COUNT(*) AS TotalTransactions\n",
        "    FROM schema_name.transaction_data\n",
        "\"\"\").show()\n",
        "\n",
        "# Profile customer locations\n",
        "spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        CustomerLocation,\n",
        "        COUNT(*) AS NumTransactions\n",
        "    FROM schema_name.transaction_data\n",
        "    GROUP BY CustomerLocation\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "w7MhqebqxPZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging the data\n",
        "merged_df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        t.TransactionID,\n",
        "        t.TransactionAmount,\n",
        "        t.TransactionDate,\n",
        "        c.CustomerID,\n",
        "        c.CustomerName,\n",
        "        c.Email,\n",
        "        c.CustomerLocation\n",
        "    FROM schema_name.transaction_data t\n",
        "    JOIN schema_name.customer_data c\n",
        "    ON t.CustomerID = c.CustomerID\n",
        "\"\"\")\n",
        "merged_df.show()"
      ],
      "metadata": {
        "id": "27LXPKoxyQXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "grant all PRIVILEGES on table finance_data_catalog.Transaction_data to 'DataEngineers';\n",
        "\n",
        "%sql\n",
        "grant all PRIVILEGES on table finance_data_catalog.Customer_data to 'DataEngineers';\n",
        "\n",
        "%sql\n",
        "grant select on table finance_data_catalog.Customer_data to 'DataAnalysts';\n",
        "\n",
        "%sql\n",
        "grant select(TransactionID,TransactionDate,TransactiionAmount) on table finance_data_catalog.Customer_data to 'DataEngineers';"
      ],
      "metadata": {
        "id": "6gAt-HHEzXVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark.sql('''SELECT COUNT(*)\n",
        "FROM schema_name.transaction_data\n",
        "WHERE TransactionAmount < 0;\n",
        "''').show()\n",
        "\n",
        "spark.sql('''\n",
        "SELECT COUNT(*)\n",
        "FROM schema_name.customer_data\n",
        "WHERE Email NOT LIKE '%_@__%.__%';\n",
        "''').show()"
      ],
      "metadata": {
        "id": "vvrtLslp53Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"vacuum finance_data_catalog.transaction_data retain 168 hours\")\n",
        "spark.sql(\"vacuum finance_data_catalog.customer_data retain 168 hours\")"
      ],
      "metadata": {
        "id": "OKsKCDLTAG1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task -2**"
      ],
      "metadata": {
        "id": "1DZJNI4cAwr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "create database if not exists corporate_data_catalog\n",
        "\n",
        "%sql\n",
        "use corporate_data_catalog\n",
        "\n",
        "%sql\n",
        "create table if not exists sales_data(SalesID String, CustomerID String,SalesAmount INT, SalesDate DATE);\n",
        "\n",
        "%sql\n",
        "create table if not exists hr_data (EmployeeID STRING, EmployeeName String,Department String, Salary INT);\n",
        "\n",
        "%sql\n",
        "create table if not exists finance_data(InvoiceID String,VendorID String,InvoiceAmount INT,PaymentDate INT);\n"
      ],
      "metadata": {
        "id": "0ACkWMAYBIhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tagging sensitive data\n",
        "%sql\n",
        "CREATE TAG SensitiveData;\n",
        "\n",
        "%sql\n",
        "ALTER TABLE hr_data.employee ADD TAG SensitiveData ON COLUMN Salary;\n",
        "\n",
        "%sql\n",
        "ALTER TABLE finance_data.invoice ADD TAG SensitiveData ON COLUMN InvoiceAmount;\n",
        "\n",
        "spark.sql(''')\n"
      ],
      "metadata": {
        "id": "gtoHm87RCJ3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provisioning\n",
        "spark.sql('''\n",
        "select max(SalesAmount) as max_sales,sum(SalesAmount) as total_sales from schema_name.sales_data\n",
        "'''\n",
        ")\n",
        "spark.sql('''\n",
        "select max(Salary) as max_salary,sum(Salary) as total_salary from schema_name.hr_data\n",
        "''')\n",
        "spark.sql('''\n",
        "select max(InvoiceAmount) as max_invoice,sum(InvoiceAmount) as total_invoice from schema_name.finance_data\n",
        "''')"
      ],
      "metadata": {
        "id": "f5c7u9KRUCiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# privileges to different persons according to their roles\n",
        "%sql\n",
        "grant all PRIVILEGES on table corporate_data_catalog.sales_data to 'SalesTeam';\n",
        "\n",
        "%sql\n",
        "revoke all PRIVILEGES on table corporate_data_catalog.hr_data from 'SalesTeam';\n",
        "\n",
        "%sql\n",
        "revoke all PRIVILEGES on table corporate_data_catalog.finance_data from 'SalesTeam;\n",
        "\n",
        "%sql\n",
        "grant select PRIVILEGE on table corporate_data_catalog.hr_data to 'HRTeam';\n",
        "\n",
        "%sql\n",
        "grant update privilege on table corporate_data_catalog.hr_data to 'HRTeam';\n",
        "\n",
        "%sql\n",
        "grant all privileges on table corporate_data_catalog.finance_data to 'FinanceTeam';\n",
        "\n",
        "#column level security\n",
        "\n",
        "%sql\n",
        "grant select(SalesID,SalesAmount,SalesDate) on table corporate_data_catalog.sales_data to 'SalesTeam';\n",
        "\n",
        "%sql\n",
        "grant select(EmployeeID,EmployeeName,Department) on table corporate_data_catalog.hr_data to 'HRTeam\n",
        "\n",
        "%sql\n",
        "grant select(InvoiceID,InvoiceAmount,PaymentDate) on table corporate_data_catalog.finance_data to 'FinanceTeam';\n"
      ],
      "metadata": {
        "id": "BwCi-KrpVCMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data quality assurance\n",
        "%sql\n",
        "ALTER TABLE sales_data.sales ADD CONSTRAINT check_sales_amount_positive CHECK (SalesAmount > 0);\n",
        "\n",
        "%sql\n",
        "ALTER TABLE hr_data.employee ADD CONSTRAINT check_salary_positive CHECK (Salary > 0);\n",
        "\n",
        "%sql\n",
        "ALTER TABLE finance_data.invoice ADD CONSTRAINT check_invoice_amount_positive CHECK (InvoiceAmount > 0);\n"
      ],
      "metadata": {
        "id": "hdbVVciCcbHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"optimize corporate_data_catalog.sales_data\")\n",
        "spark.sql(\"optimize corporate_data_catalog.finance_data\")\n",
        "spark.sql(\"optimize corporate_data_catalog.hr_data\")\n",
        "spark.sql(\"vacuum corporate_data_catalog.sales_data retain 168 hours\")\n",
        "spark.sql(\"vacuum corporate_data_catalog.finance_data retain 168 hours\")\n",
        "spark.sql(\"vacuum corporate_data_catalog.hr_data retain 168 hours\")"
      ],
      "metadata": {
        "id": "8cg68Ff8dHPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task-3**`"
      ],
      "metadata": {
        "id": "T63sSjW8d9lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "create database if not exists enterprise_data_catalog\n",
        "\n",
        "%sql\n",
        "create table if not exists marketing_data(CampaignID String,CampaignName String, Budget INT, StartDate Date);\n",
        "\n",
        "%sql\n",
        "create table if not exists  operations_data( OrderID String,ProductID String, Quantity INT, ShippingStatus String);\n",
        "\n",
        "%sql\n",
        "create table if not exists it_data(IncidentID String,ReportedBy String, IssueType String, ResolutionTime DateTime);\n"
      ],
      "metadata": {
        "id": "-_Q7wueRdn2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "%sql\n",
        "CREATE TAG SensitiveData1;\n",
        "\n",
        "%sql\n",
        "ALTER TABLE enterprise_data_catalog.marketing_data ADD TAG SensitiveData1 ON COLUMN Budget;\n",
        "\n",
        "%sql\n",
        "ALTER TABLE enterprise_data_catalog.marketing_data ADD TAG SensitiveData1 ON COLUMN Budget;\n",
        "\n",
        "%sql\n",
        "ALTER TABLE enterprise_data_catalog.operations_data ADD TAG SensitiveData1 ON COLUMN Quantity;\n",
        "\n",
        "%sql\n",
        "ALTER TABLE enterprise_data_catalog.it_data ADD TAG SensitiveData1 ON COLUMN ResolutionTime;\n"
      ],
      "metadata": {
        "id": "b3Dus1sHfI8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#provisioningb\n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "# Add year and month columns\n",
        "df_marketing = df_marketing.withColumn(\"Year\", year(df_marketing[\"StartDate\"]))\n",
        "df_marketing = df_marketing.withColumn(\"Month\", month(df_marketing[\"StartDate\"]))\n",
        "\n",
        "# Group by year and month for budget trends\n",
        "df_marketing.groupBy(\"Year\", \"Month\").agg({\"Budget\": \"sum\"}).orderBy(\"Year\", \"Month\").show()\n",
        "\n",
        "# Load operational data\n",
        "df_operational = spark.table(\"operational_data\")\n",
        "\n",
        "# Count occurrences of each shipping status\n",
        "df_operational.groupBy(\"ShippingStatus\").count().orderBy(\"count\", ascending=False).show()\n",
        "# Add year and month columns\n",
        "df_operational = df_operational.withColumn(\"Year\", year(df_operational[\"ShippingDate\"]))\n",
        "df_operational = df_operational.withColumn(\"Month\", month(df_operational[\"ShippingDate\"]))\n",
        "\n",
        "# Group by year and month to see shipping trends\n",
        "df_operational.groupBy(\"Year\", \"Month\").count().orderBy(\"Year\", \"Month\").show()\n",
        "# Filter and count delayed shipments\n",
        "delayed_shipments = df_operational.filter(df_operational[\"ShippingStatus\"] == \"Delayed\")\n",
        "delayed_count = delayed_shipments.count()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SY0WkM2Af7rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "grant all privileges on table enterprise_data_catalog.marketing_data to 'MarketingTeam';\n",
        "\n",
        "%sql\n",
        "revoke all privileges on table enterprise_data_catalog.it_data from 'MarketingTeam';\n",
        "\n",
        "%sql\n",
        "revoke all privileges on table enterprise_data_catalog.operations_data from 'MarketingTeam';\n",
        "\n",
        "%sql\n",
        "grant all privileges on table enterprise_data_catalog.operations_data to 'OperationsTeam';\n",
        "\n",
        "%sql\n",
        "grant select privileges on table enterprise_data_catalog.matketing_data to 'OperationsTeam';\n",
        "\n",
        "%sql\n",
        "grant all privileges on table enterprise_data_catalog.it_data to 'ITTeam';\n",
        "\n",
        "%sql\n",
        "grant update privileges on table enterprise_data_catalog.it_data to 'ITTeam';\n",
        "\n",
        "#column level security\n",
        " grant select(CampaignID,CampaignName,StartDate) on table enterprise_data_catalog.marketing_data to 'MarketingTeam';\n",
        "\n",
        "%sql\n",
        "grant select(OrderID,ProductID,ShippingStatus) on table enterprise_data_catalog.operations_data to 'OperationsTeam';\n",
        "\n",
        "%sql\n",
        "grant select(IncidentID,ReportedBy,IssueType) on table enterprise_data_catalog.it_data to 'ITTeam';\n"
      ],
      "metadata": {
        "id": "b76ckoIFhHOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "ALTER TABLE marketing_data ADD CONSTRAINT check_budget_positive CHECK (Budget > 0);\n",
        "\n",
        "df_marketing = spark.table(\"marketing_data\")\n",
        "# Validate that all campaign budgets are greater than zero\n",
        "invalid_budget = df_marketing.filter(df_marketing[\"Budget\"] <= 0)\n",
        "invalid_budget.show()\n",
        "\n",
        "%sql\n",
        "ALTER TABLE operations_data ADD CONSTRAINT check_shipping_status CHECK (ShippingStatus IN ('Pending', 'Shipped', 'Delivered'));\n",
        "df_operations = spark.table(\"operations_data\")\n",
        "\n",
        "%sql\n",
        "ALTER TABLE it_data ADD CONSTRAINT check_resolution_time_non_negative CHECK (ResolutionTime >= 0);\n",
        "\n",
        "df_it = spark.table(\"it_data\")\n",
        "invalid_resolution_time = df_it.filter(df_it[\"ResolutionTime\"] < 0)\n"
      ],
      "metadata": {
        "id": "ZSudZXjyiUy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"optimize enterprise_data_catalog.marketing_data\")\n",
        "spark.sql(\"optimize enterprise_data_catalog.operations_data\")\n",
        "spark.sql(\"optimize enterprise_data_catalog.it_data\")\n",
        "\n",
        "spark.sql(''' vacuum enterprise_data_catalog.marketing_data retain 168 hours''')\n",
        "spark.sql(''' vacuum enterprise_data_catalog.operations_data retain 168 hours''')\n",
        "spark.sql(''' vacuum enterprise_data_catalog.it_data retain 168 hours''')\n"
      ],
      "metadata": {
        "id": "0hYKi_CskhFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task-4**"
      ],
      "metadata": {
        "id": "uxHAFDCFkwxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  df_weather=pd.read_csv(\"weather.csv\")\n",
        "  print(df)\n",
        "except Exception as e:\n",
        "  print(\"The file doesnt exists\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder.appName(\"WeatherData\").getOrCreate()\n",
        "\n",
        "dbutils.fs.cp(\"file:workspace/shared/weather.csv\",\"dbfs:filestore/weather.csv\")\n",
        "df_weather=spark.read.format(\"csv\").load(\"dbfs:filestore/weather.csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\")\n",
        "df_weather.write.format(\"delta\").save(\"dbf;/delta/weather_delta\")\n",
        "#Cleaning Data\n",
        "df_weather=spark.read.format(\"delta\").load(\"dbfs:/delta/weather_delta\")\n",
        "df_weather1=df_weather.drop.isna()\n",
        "df_weather1.write.format(\"delta\").save(\"dbfs:/delta/weather_delta_new\")\n"
      ],
      "metadata": {
        "id": "Zkjo_pq9kz1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import DLT\n",
        "\n",
        "@dlt.table\n",
        "def transformed_weather_data():\n",
        "    df_cleaned = dlt.read(\"df_weather1\")\n",
        "    # Perform transformations\n",
        "    try:\n",
        "        df_transformed = (\n",
        "            df_cleaned\n",
        "            .withColumn(\"avg_Humidity\", F.avg(F.col(\"Humidity\")).over())\n",
        "            .withColumn(\"avg_Temperature\", F.avg(F.col(\"Temperature\")).over())\n",
        "        )\n",
        "        dlt.log(\"Data transformation successful\")\n",
        "        return df_transformed\n",
        "    except Exception as e:\n",
        "        dlt.log(f\"Data transformation failed: {str(e)}\")\n",
        "        raise Exception(f\"Failed to transform weather data: {e}\")"
      ],
      "metadata": {
        "id": "qywdqKl96iiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  df_transactions=pd.read_csv(\"transactions.csv\")\n",
        "  print(df_transactions)\n",
        "except Exception as e:\n",
        "  print(\"The file doesnt exists\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder.appName(\"transactions\").getOrCreate()\n",
        "\n",
        "df_transactions=spark.read.format(\"csv\").load(\"dbfs:filestore/transactions.csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\")\n",
        "df_transactions.write.format(\"delta\").save(\"dbfs:/delta/transactions_delta\")\n",
        "\n",
        "dbutils.fs.cp(\"file:workspace/shared/transactions.csv\",\"dbfs:filestore/transactions.csv\")\n",
        "# Task 2: Data Cleaning\n",
        "@dlt.table\n",
        "def clean_weather_data():\n",
        "    \"\"\"\n",
        "    Clean weather data by removing null values.\n",
        "    \"\"\"\n",
        "    df_weather = dlt.read(\"df_transactions\")\n",
        "    try:\n",
        "        df_cleaned = df_weather.dropna()\n",
        "        dlt.log(\"Data cleaning successful.\")\n",
        "        return df_cleaned\n",
        "    except Exception as e:\n",
        "        dlt.log(f\"Data cleaning failed: {str(e)}\")\n",
        "        raise Exception(f\"Failed to clean weather data: {e}\")\n",
        "\n",
        "\n",
        "# Task 3: Data Transformation\n",
        "@dlt.table\n",
        "def transform_weather_data():\n",
        "    \"\"\"\n",
        "    Transform the cleaned weather data by adding average humidity and temperature columns.\n",
        "    \"\"\"\n",
        "    df_cleaned = dlt.read(\"clean_weather_data\")\n",
        "    try:\n",
        "        # Add new columns with the average of Humidity and Temperature\n",
        "        df_transformed = (\n",
        "            df_cleaned\n",
        "            .withColumn(\"avg_Humidity\", F.avg(F.col(\"Humidity\")).over())\n",
        "            .withColumn(\"avg_Temperature\", F.avg(F.col(\"Temperature\")).over())\n",
        "        )\n",
        "        dlt.log(\"Data transformation successful.\")\n",
        "        return df_transformed\n",
        "    except Exception as e:\n",
        "        dlt.log(f\"Data transformation failed: {str(e)}\")\n",
        "        raise Exception(f\"Failed to transform weather data: {e}\")\n"
      ],
      "metadata": {
        "id": "s-B3Frc48EQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.cp(\"file:workspace/shared/inventory.csv\",\"dbfs:/filestore/inventory.csv\")\n",
        "try:\n",
        "  df_inventory=pd.read_csv(\"inventory.csv\")\n",
        "  print(df_inventory)\n",
        "except Exception as e:\n",
        "  print(\"The file doesnt exists\")\n",
        "df_inventory=spark.read.format(\"csv\").load(\"dbfs:/filestore/inventory.csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\")\n",
        "df_inventory.write.format(\"delta\").save(\"dbfs:/delta/inventory_delta\")\n",
        "\n",
        "@dlt.table\n",
        "def clean_inventory_data():\n",
        "    \"\"\"\n",
        "    Clean inventory data by handling missing values or duplicates.\n",
        "    \"\"\"\n",
        "    df_inventory = dlt.read(\"ingest_inventory_data\")\n",
        "    try:\n",
        "        # Drop rows with null values\n",
        "        df_cleaned = df_inventory.dropna()\n",
        "        # Optionally, you can remove duplicates\n",
        "        df_cleaned = df_cleaned.dropDuplicates()\n",
        "        dlt.log(\"Inventory data cleaning successful.\")\n",
        "        return df_cleaned\n",
        "    except Exception as e:\n",
        "        dlt.log(f\"Inventory data cleaning failed: {str(e)}\")\n",
        "        raise Exception(f\"Failed to clean inventory data: {e}\")\n",
        "\n",
        "@dlt.table\n",
        "def transform_inventory_data():\n",
        "    \"\"\"\n",
        "    Transform the cleaned inventory data by performing aggregations or calculating new columns.\n",
        "    \"\"\"\n",
        "    df_cleaned = dlt.read(\"clean_inventory_data\")\n",
        "    try:\n",
        "        # Example transformation: Adding a new column for total value based on quantity and price\n",
        "        df_transformed = df_cleaned.withColumn(\"total_value\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "        dlt.log(\"Inventory data transformation successful.\")\n",
        "        return df_transformed\n",
        "    except Exception as e:\n",
        "        dlt.log(f\"Inventory data transformation failed: {str(e)}\")\n",
        "        raise Exception(f\"Failed to transform inventory data: {e}\")\n"
      ],
      "metadata": {
        "id": "8ibwQJijCUtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.cp(\"file:workspace/shared/employ.csv\",\"dbfs:/filestore/employ.csv\")\n",
        "try:\n",
        "  df_employ=pd.read_csv(\"employ.csv\")\n",
        "  print(df_employ)\n",
        "except Exception as e:\n",
        "  print(\"The file doesnt exists\")\n",
        "df_employ=spark.read.format(\"csv\").load(\"dbfs:/filestore/employ.csv\").option(\"header\",\"True\").option(\"inferSchema\",\"True\")\n",
        "df_employ.write.format(\"delta\").save(\"dbfs:/delta/inventory_delta\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as spark_sum, dayofmonth, month, year\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AttendanceSummary\").getOrCreate()\n",
        "attendance_df = spark.read.format(\"delta\").load(\"dbfs:/delta/attendance_data\")\n",
        "\n",
        "current_month = 9\n",
        "current_year = 2024\n",
        "\n",
        "filtered_df = attendance_df.filter(month(col(\"date\")) == current_month) & (year(col(\"date\")) == current_year))\n",
        "total_hours_df = filtered_df.groupBy(\"employee_id\").agg(spark_sum(\"hours_worked\").alias(\"total_hours\"))\n",
        "overtime_df = filtered_df.filter(col(\"hours_worked\") > 8)\n",
        "total_hours_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/delta/attendance_summary\")\n",
        "overtime_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/delta/overtime_summary\")\n",
        "\n",
        "import dlt\n",
        "from pyspark.sql.functions import col, sum as spark_sum, dayofmonth, month, year\n",
        "# Step 1: Data Ingestion\n",
        "@dlt.table\n",
        "def ingest_attendance_data():\n",
        "    return spark.read.format(\"delta\").load(\"dbfs:/delta/attendance_data\")\n",
        "# Step 2: Data Cleaning\n",
        "@dlt.table\n",
        "def clean_attendance_data():\n",
        "    attendance_df = dlt.read(\"ingest_attendance_data\")\n",
        "    # Example cleaning: Drop rows with missing values\n",
        "    return attendance_df.dropna()\n",
        "# Step 3: Attendance Summary\n",
        "@dlt.table\n",
        "def attendance_summary():\n",
        "    attendance_df = dlt.read(\"clean_attendance_data\")\n",
        "\n",
        "    current_month = 9  # September\n",
        "    current_year = 2024\n",
        "\n",
        "    filtered_df = attendance_df.filter(\n",
        "        (month(col(\"date\")) == current_month) & (year(col(\"date\")) == current_year)\n",
        "    )\n",
        "\n",
        "    # Calculate total hours worked by each employee\n",
        "    return filtered_df.groupBy(\"employee_id\").agg(spark_sum(\"hours_worked\").alias(\"total_hours\"))\n",
        "\n",
        "# Step 4: Overtime Detection\n",
        "@dlt.table\n",
        "def overtime_summary():\n",
        "    attendance_df = dlt.read(\"clean_attendance_data\")\n",
        "    return attendance_df.filter(col(\"hours_worked\") > 8)\n",
        "\n",
        "# View the history of changes made to the Delta table\n",
        "spark.sql(\"DESCRIBE HISTORY delta.`dbfs:/delta/attendance_data`\").show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "AXDh0hFiEwf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}