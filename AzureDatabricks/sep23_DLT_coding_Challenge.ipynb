{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["NXNHtF6hacOe","nDZP6xuLeX3r","StvyOImrGE9J","lGD6eX1iJFQt"],"authorship_tag":"ABX9TyN2Sakc2w9KnPKUKwF9cZkN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"TzqKuy1LaaJ4"}},{"cell_type":"markdown","source":["## Task 1"],"metadata":{"id":"NXNHtF6hacOe"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEAA_hI7aOyl","executionInfo":{"status":"ok","timestamp":1727070421787,"user_tz":-330,"elapsed":58122,"user":{"displayName":"Swetha .R","userId":"11283228655930516066"}},"outputId":"7c44de99-f50f-4d77-bf46-197042bcf22a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from Pyspark) (0.10.9.7)\n","Building wheels for collected packages: Pyspark\n","  Building wheel for Pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=84b873268497fbf2a923a2d55db8a58d92df4c736103835b738ed82bddfc2f9a\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built Pyspark\n","Installing collected packages: Pyspark\n","Successfully installed Pyspark-3.5.2\n"]}],"source":["! pip install Pyspark"]},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/Workspace/Shared/customer_transaction.csv\", \"dbfs:/FileStore/streaming/input/customer_transaction.csv\")"],"metadata":{"id":"gTO6WViv-Gx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["create an ETL Pipeline using DLT (Python)"],"metadata":{"id":"478nKiB8_E6g"}},{"cell_type":"code","source":["import dlt\n","from pyspark.sql.functions import col\n","\n","@dlt.table\n","def raw_transactions():\n","\n","    return spark.read.csv(\"dbfs:/FileStore/streaming/input/customer_transaction.csv\", header=True)\n","\n","@dlt.table\n","def transformed_transactions():\n","    df = dlt.read(\"raw_transactions\")\n","    df_transformed = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n","                       .filter(col(\"Quantity\") > 1)\n","\n","    return df_transformed\n"],"metadata":{"id":"gmQLue4tarSG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Create an ETL Pipeline using DLT (SQL)"],"metadata":{"id":"OAgfcLpC_JXW"}},{"cell_type":"code","source":["##  Step 1: Define the raw transactions table\n","\n","df_transactions = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/streaming/input/customer_transactions.csv\")\n","\n","df_transactions.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/customer_transactions\")\n","\n","\n","\n"],"metadata":{"id":"Ctf-fL9xeAI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%sql\n","\n","CREATE OR REFRESH LIVE TABLE transformed_transactions AS\n","SELECT\n","    TransactionID,\n","    TransactionDate,\n","    CustomerID,\n","    Product,\n","    Quantity,\n","    Price,\n","    Quantity * Price AS TotalAmount\n","FROM delta.\"/delta/customer_transactions\";\n","\n"],"metadata":{"id":"fc-E22IzAlHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## task 2"],"metadata":{"id":"nDZP6xuLeX3r"}},{"cell_type":"markdown","source":[" Delta Lake Operations - Read, Write, Update, Delete, Merge"],"metadata":{"id":"LenwzyoTCPVo"}},{"cell_type":"markdown","source":["1. Read Data from Delta Lake:"],"metadata":{"id":"RCy-F16ZCXX3"}},{"cell_type":"code","source":["# using python\n","df = spark.read.format(\"delta\").load(\"/delta/customer_transactions\")\n","\n","df.show(5)\n","\n"],"metadata":{"id":"E0z6drhUCW8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%sql\n","SELECT * FROM delta.'/delta/customer_transactions' LIMIT 5;"],"metadata":{"id":"K-yDNSCheZ1v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Write Data to Delta Lake:"],"metadata":{"id":"MAgR4mWBC6Kk"}},{"cell_type":"code","source":["# Create new transactions DataFrame\n","new_data = [\n","    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n","    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n","]\n","\n","new_transactions_df = spark.createDataFrame(new_data, schema=[\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n","\n","\n","new_transactions_df.write.format(\"delta\").mode(\"append\").save(\"/delta/customer_transactions\")\n"],"metadata":{"id":"nyxukzUsDEtK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Update Data in Delta Lake:"],"metadata":{"id":"5YNujL6pDZNm"}},{"cell_type":"code","source":["## using python\n","\n","from delta.tables import DeltaTable\n","delta_table = DeltaTable.forPath(spark, \"/delta/customer_transactions\")\n","\n","delta_table.update(\n","    condition = \"Product = 'Laptop'\",\n","    set = { \"Price\": \"1300\" }\n",")\n","\n","df_updated = spark.read.format(\"delta\").load(\"/delta/customer_transactions\")\n","df_updated.filter(\"Product = 'Laptop'\").show()\n"],"metadata":{"id":"Y0loplU7D0C1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## using sql\n","UPDATE delta.`/delta/customer_transaction`\n","SET Price = 1300\n","WHERE Product = 'Laptop';"],"metadata":{"id":"tQx2b-RqEDBU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Delete Data from Delta Lake"],"metadata":{"id":"7msUpPiTERoO"}},{"cell_type":"code","source":["## using python\n","delta_table.delete(\"Quantity < 3\")\n","\n","df_after_delete = spark.read.format(\"delta\").load(\"/delta/customer_transactions\")\n","df_after_delete.show()\n"],"metadata":{"id":"CeOttRHtEcJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## using sql\n","DELETE FROM delta.`/delta/customer_transactions`\n","WHERE Quantity < 3;"],"metadata":{"id":"zJ4I970-E2zQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Merge Data into Delta Lake:"],"metadata":{"id":"AdYXlfaiE_7c"}},{"cell_type":"code","source":["# Create DataFrame for merge\n","merge_data = [\n","    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1250),\n","    (8, \"2024-09-08\", \"C007\", \"Charger\", 2, 30)\n","]\n","\n","merge_df = spark.createDataFrame(merge_data, schema=[\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n"],"metadata":{"id":"18y4ZPDYFGKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## using python\n","\n","merge_df.createOrReplaceTempView(\"updates\")\n","\n","# Merge statement\n","delta_table.alias(\"target\").merge(\n","    updates.alias(\"source\"),\n","    \"target.TransactionID = source.TransactionID\"\n",").whenMatchedUpdate(\n","    condition=\"target.TransactionID = source.TransactionID\",\n","    set={\"Price\": \"source.Price\", \"Quantity\": \"source.Quantity\", \"TransactionDate\": \"source.TransactionDate\"}\n",").whenNotMatchedInsertAll().execute()\n"],"metadata":{"id":"skRuoHVGFXwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## using sql\n","%sql\n","MERGE INTO delta.`/delta/customer_transactions` AS target\n","USING (SELECT * FROM VALUES\n","    (1, '2024-09-01', 'C001', 'Laptop', 1, 1250),\n","    (8, '2024-09-08', 'C007', 'Charger', 2, 30)\n",") AS source (TransactionID, TransactionDate, CustomerID, Product, Quantity, Price)\n","ON target.TransactionID = source.TransactionID\n","WHEN MATCHED THEN\n","    UPDATE SET target.Price = source.Price, target.Quantity = source.Quantity, target.TransactionDate = source.TransactionDate\n","WHEN NOT MATCHED THEN\n","    INSERT *\n"],"metadata":{"id":"vOUaJAfNFtIz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## task 3"],"metadata":{"id":"StvyOImrGE9J"}},{"cell_type":"markdown","source":["Delta Lake - History, Time Travel, and Vacuum"],"metadata":{"id":"DzwmLiXvGHLT"}},{"cell_type":"markdown","source":["1. View Delta Table History:"],"metadata":{"id":"CTqzqySPGPs0"}},{"cell_type":"code","source":["# Check the transaction history using PySpark\n","delta_table.history().show()\n","# Check file details using sql\n","spark.sql(\"DESCRIBE DETAIL delta.`/delta/customer_transactions`\").show()"],"metadata":{"id":"yJZSX_XXGUOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perform Time Travel"],"metadata":{"id":"VtwOJGsFG9yl"}},{"cell_type":"code","source":["# Load the table as it was 5 versions ago\n","df_version_5 = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"/delta/customer_transactions\")\n","df_version_5.show()\n"],"metadata":{"id":"tzwoIgNBHC6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve the state of the table at a specific timestamp\n","timestamp = \"2024-09-01T12:00:00\"\n","df_at_time = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp).load(\"/delta/customer_transactions\")\n","\n","df_at_time.show()\n"],"metadata":{"id":"i_I73EtWHSiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## sql\n","%sql\n","SELECT * FROM delta.`/delta/orders` VERSION AS OF 5;\n","\n","%sql\n","SELECT * FROM delta.`/delta/orders` TIMESTAMP AS OF '2024-09-01T12:00:00';\n"],"metadata":{"id":"kw3ZeIpSHU9j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Vacuum the Delta Table:"],"metadata":{"id":"gRbTF2HaHxQ-"}},{"cell_type":"code","source":["\n","delta_table.vacuum(retentionHours=168)\n"],"metadata":{"id":"jGj-tJKvIPJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark.sql(\"VACUUM delta.`/delta/customer_transactions` RETAIN 168 HOURS\")"],"metadata":{"id":"5uZUOLEHH782"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Converting Parquet Files to Delta Files"],"metadata":{"id":"QIWLeKsrIToe"}},{"cell_type":"code","source":["\n","df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/streaming/input/customer_transactions.csv\")\n","\n","df_csv.write.format(\"parquet\").mode(\"overwrite\").save(\"/parquet/customer_transactions\")\n"],"metadata":{"id":"cGJ1-Gv3IZ0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the Parquet table to Delta\n","spark.read.format(\"parquet\").load(\"/parquet/customer_transactions/\").write.format(\"delta\").mode(\"overwrite\").save(\"/delta/orders_converted\")\n"],"metadata":{"id":"C4OGSsyXIs23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Convert a Parquet table to Delta using SQL\n","%sql\n","CONVERT TO DELTA parquet.`/parquet/customer_transactions`;\n"],"metadata":{"id":"b0ydEnAQI44O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## task 4"],"metadata":{"id":"lGD6eX1iJFQt"}},{"cell_type":"markdown","source":["Implementing Incremental Load Pattern using Delta Lake"],"metadata":{"id":"CzIfqIxuJHRV"}},{"cell_type":"markdown","source":[],"metadata":{"id":"7wkeNd26JMdf"}},{"cell_type":"code","source":["\n","initial_data = [\n","    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1200),\n","    (2, \"2024-09-02\", \"C002\", \"Tablet\", 2, 300),\n","    (3, \"2024-09-03\", \"C001\", \"Headphones\", 5, 50)\n","]\n","\n","columns = [\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n","df_initial = spark.createDataFrame(initial_data, columns)\n","\n","df_initial.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/transactions\")\n"],"metadata":{"id":"6iQTsa8GJM7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Incremental data"],"metadata":{"id":"YZ9BVBhoLA8m"}},{"cell_type":"code","source":["\n","incremental_data = [\n","    (4, \"2024-09-04\", \"C003\", \"Smartphone\", 1, 800),\n","    (5, \"2024-09-05\", \"C004\", \"Smartwatch\", 3, 200),\n","    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n","    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n","]\n","\n","df_incremental = spark.createDataFrame(incremental_data, columns)\n"],"metadata":{"id":"GbcfCFaSKesm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement Incremental Load"],"metadata":{"id":"VazkoosBLD3V"}},{"cell_type":"code","source":["\n","new_transactions = df_incremental.filter(col(\"TransactionDate\") > \"2024-09-03\")\n","\n","new_transactions.write.format(\"delta\").mode(\"append\").save(\"/delta/transactions\")\n"],"metadata":{"id":"3ITs6lHfLJxr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Monitor incremental Load"],"metadata":{"id":"I8g48NPhLPsg"}},{"cell_type":"code","source":["\n","%sql\n","DESCRIBE HISTORY delta.`/delta/transactions`\n"],"metadata":{"id":"qN54r_ONLPMl"},"execution_count":null,"outputs":[]}]}