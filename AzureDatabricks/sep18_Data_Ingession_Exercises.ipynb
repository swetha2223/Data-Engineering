{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["0_0uae_5W_OZ","cmw6wM_AXGjT","L6PKGhbOQbPe","-tDyau6IUY1W","lNWQh_KiXzsl"],"authorship_tag":"ABX9TyM7SpSZvXFOpl8vQk4spfJv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Assignment-1"],"metadata":{"id":"0_0uae_5W_OZ"}},{"cell_type":"markdown","source":["Task 1 - Raw Data Ingestion"],"metadata":{"id":"unz0AEQ0S9jl"}},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MD3i2vXiTgQc","executionInfo":{"status":"ok","timestamp":1726999501495,"user_tz":-330,"elapsed":59837,"user":{"displayName":"Swetha .R","userId":"11283228655930516066"}},"outputId":"0a1aea5d-3cce-44ab-80ff-94bcdd3d8f5f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=96099af5bcedaa2bfeef645d800b86ae8d0ead519d7e502ebbccb2775a2196da\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.2\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import input_file_name\n","from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType\n","import os\n","\n","spark = SparkSession.builder.appName(\"WeatherDataIngestion\").getOrCreate()\n","\n","schema = StructType([\n","    StructField(\"City\", StringType(), True),\n","    StructField(\"Date\", DateType(), True),\n","    StructField(\"Temperature\", FloatType(), True),\n","    StructField(\"Humidity\", FloatType(), True)\n","])\n","# Define path to the raw data\n","raw_data_path = \"/content/sample_data/weather_data.csv\"\n","delta_table_path = \"/content/samplw_data/delta/weather_raw\"\n","\n","weather_df = spark.read.csv(raw_data_path, schema=schema, header=True).withColumn(\"file_name\", input_file_name())\n","\n","if os.path.exists(raw_data_path):\n","    try:\n","\n","        weather_df = spark.read.csv(raw_data_path, schema=schema, header=True)\n","\n","        weather_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n","        print(\"Data loaded and saved as Delta table.\")\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","else:\n","    print(f\"File not found: {raw_data_path}\")"],"metadata":{"id":"4PLTdCabS9MY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 2 - Data Cleaning"],"metadata":{"id":"1nWGtaCJT8G3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRCnCMctS7vr"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"WeatherDataCleaning\").getOrCreate()\n","\n","\n","raw_delta_table_path = \"/content/sample_data/delta/weather_raw\"\n","cleaned_delta_table_path = \"/content/sample_data/delta/weather_cleaned\"\n","\n","raw_weather_df = spark.read.format(\"delta\").load(raw_delta_table_path)\n","raw_weather_df.show()\n","\n","cleaned_weather_df = raw_weather_df.na.drop()\n","cleaned_weather_df.show()\n","\n","cleaned_weather_df.write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_table_path)\n","print(\"Data cleaned and saved to a new Delta table.\")\n"]},{"cell_type":"markdown","source":["Task 3: Data Transformation"],"metadata":{"id":"oPNoOvbDVDPQ"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import avg\n","\n","spark = SparkSession.builder.appName(\"WeatherDataTransformation\").getOrCreate()\n","\n","raw_delta_table_path = \"/content/sample_data/delta/weather_raw\"\n","cleaned_delta_table_path = \"/content/sample_data/delta/weather_cleaned\"\n","\n","\n","cleaned_weather_df = spark.read.format(\"delta\").load(cleaned_delta_table_path)\n","cleaned_weather_df.show()\n","\n","transformed_weather_df = cleaned_weather_df.groupBy(\"City\").agg(\n","    avg(\"Temperature\").alias(\"AvgTemperature\"),\n","    avg(\"Humidity\").alias(\"AvgHumidity\")\n",")\n","\n","transformed_weather_df.show()\n","\n","transformed_weather_df.write.format(\"delta\").mode(\"overwrite\").save(transformed_delta_table_path)\n","print(\"Data transformed and saved to a new Delta table.\")\n"],"metadata":{"id":"85g4k00yVInB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task-4 Creating Pipelines\n","\n"],"metadata":{"id":"Y1tIR9TpV2p5"}},{"cell_type":"markdown","source":["Pipelines are created in Azure Databricks under the section Workflow - which will connect the above 3 tasks into a single performance and process required output."],"metadata":{"id":"6VpNneg9WEOc"}},{"cell_type":"code","source":["# 2. Add Logging to track progress and errors\n","import logging\n","logging.basicConfig(filename='/path/to/pipeline_log.log', level=logging.INFO)\n","\n","try:\n","  logging.info(f'Successfully executed {notebook}')\n","\n","except Exception as e:\n","  logging.error(f'Failed to execute {notebook}: {e}')\n"],"metadata":{"id":"kou4X8fVOtLJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Additional Task - Error Handling"],"metadata":{"id":"UVdUMOvEOoi7"}},{"cell_type":"code","source":["import os\n","if not os.path.exists(\"dbfs:/FileStore/weather_data.csv\"):\n"," raise FileNotFoundError(\"Weather data file not found\")\n","\n","\n","try\n","except Exception as e:\n","  logging.error(f\"Error: {str(e)}\")\n","  error_df = spark.createDataFrame([(str(e),)], [\"Error\"])\n","  error_df.write.format(\"delta\").mode(\"append\").save(\"/delta/error_log\")\n","\n"],"metadata":{"id":"EsgD8gm0Wc8O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assignment -2"],"metadata":{"id":"cmw6wM_AXGjT"}},{"cell_type":"markdown","source":["Task 1 - Raw Data Ingestion"],"metadata":{"id":"zC7eLH-TXJWV"}},{"cell_type":"markdown","source":["weather_data.csv"],"metadata":{"id":"gS67ukYOLZUk"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","import os\n","\n","spark = SparkSession.builder.appName(\"Weather Data Ingestion\").getOrCreate()\n","schema = StructType([\n","    StructField(\"City\", StringType(), True),\n","    StructField(\"Date\", DateType(), True),\n","    StructField(\"Temperature\", FloatType(), True),\n","    StructField(\"Humidity\", FloatType(), True)\n","])\n","\n","file_path = \"/content/sample_data/weather_data.csv\"\n","\n","if os.path.exists(file_path):\n","\n","    weather_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n","\n","    weather_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/weather_raw\")\n","    print(\"Data ingestion completed successfully.\")\n","else:\n","    print(f\"File {file_path} does not exist.\")\n","    spark.createDataFrame([(\"File not found\",)], [\"Error\"]).write.mode(\"append\").save(\"/content/sample_data/delta/ingestion_logs\")\n"],"metadata":{"id":"sufW3X0LXT-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 2 Data Cleaning"],"metadata":{"id":"PktjvqeTMcde"}},{"cell_type":"code","source":["from pyspark.sql.functions import when, col\n","\n","weather_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/weather_raw\")\n","\n","cleaned_df = weather_df.withColumn(\n","    \"Temperature\", when(col(\"Temperature\").isNull() | (col(\"Temperature\") < -50) | (col(\"Temperature\") > 50), None).otherwise(col(\"Temperature\"))\n",").withColumn(\n","    \"Humidity\", when(col(\"Humidity\").isNull() | (col(\"Humidity\") < 0) | (col(\"Humidity\") > 100), None).otherwise(col(\"Humidity\"))\n",")\n","\n","cleaned_df = cleaned_df.dropna()\n","\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/weather_cleaned\")\n","print(\"Data cleaning completed successfully.\")\n"],"metadata":{"id":"kokxMO8LMiDw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 3 Data Transition"],"metadata":{"id":"Mwt2Gr4OPJKj"}},{"cell_type":"code","source":["from pyspark.sql.functions import avg\n","\n","cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/weather_cleaned\")\n","transformed_df = cleaned_df.groupBy(\"City\").agg(\n","    avg(\"Temperature\").alias(\"Average_Temperature\"),\n","    avg(\"Humidity\").alias(\"Average_Humidity\")\n",")\n","\n","transformed_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/weather_transformed\")\n","print(\"Data transformation completed successfully.\")\n"],"metadata":{"id":"sx-vEhUMPcni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 4 Build and Run a Pipeline"],"metadata":{"id":"ZX6iHxKUPp2n"}},{"cell_type":"code","source":["import subprocess\n","import logging\n","\n","\n","logging.basicConfig(filename='/content/sample_data/logs/pipeline_log.log', level=logging.INFO)\n","notebooks = [\n","    \"/content/sample_data/delta/weather_raw\",\n","    \"/content/sample_data/delta/weather_cleaned\",\n","    \"/content/sample_data/delta/weather_transformed\"\n","]\n","\n","for notebook in notebooks:\n","    try:\n","        subprocess.run([\"databricks\", \"workspace\", \"import\", notebook], check=True)\n","        logging.info(f\"Successfully executed {notebook}\")\n","    except subprocess.CalledProcessError as e:\n","        logging.error(f\"Error occurred while executing {notebook}: {e}\")\n"],"metadata":{"id":"IQBzxFg0PwyY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assignment 3"],"metadata":{"id":"L6PKGhbOQbPe"}},{"cell_type":"markdown","source":["Task 1 Data Ingestion"],"metadata":{"id":"5dmrs6fZQfyJ"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import os\n","import logging\n","spark = SparkSession.builder.appName(\"Customer Data Ingestion\").getOrCreate()\n","file_path = \"/content/sample_data/customer_transactions.csv\"\n","\n","logging.basicConfig(filename='/content/sample_data/logs/ingestion_log.log', level=logging.INFO)\n","\n","if os.path.exists(file_path):\n","    customer_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n","\n","    customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/customer_raw\")\n","    logging.info(\"Customer data ingestion completed successfully.\")\n","else:\n","    logging.error(f\"File {file_path} does not exist.\")\n"],"metadata":{"id":"1Yc2k7pUQfMT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 2  Data Cleaning"],"metadata":{"id":"3QgIkS7UTCFs"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","customer_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_raw\")\n","cleaned_df = customer_df.dropDuplicates()\n","cleaned_df = cleaned_df.na.fill({\"TransactionAmount\": 0})\n","\n","\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/customer_cleaned\")\n","print(\"Customer data cleaning completed successfully.\")\n"],"metadata":{"id":"w-I6rd7TTPpP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 3 Data Aggregation"],"metadata":{"id":"1znQ8YAMTjS6"}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n","cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_cleaned\")\n","\n","aggregated_df = cleaned_df.groupBy(\"ProductCategory\").agg(\n","    sum(\"TransactionAmount\").alias(\"TotalTransactionAmount\")\n",")\n","\n","aggregated_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/customer_aggregated\")\n","print(\"Customer data aggregation completed successfully.\")\n"],"metadata":{"id":"jZE3iQBrTnW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 4  Pipeline Creation"],"metadata":{"id":"kMQ21F6kT2pL"}},{"cell_type":"code","source":["import subprocess\n","import logging\n","\n","logging.basicConfig(filename='/content/sample_data/logs/pipeline_log.log', level=logging.INFO)\n","notebooks = [\n","    \"/content/sample_data/delta/customer_raw\",\n","    \"/content/sample_data/delta/customer_cleaned\",\n","    \"/content/sample_data/delta/customer_aggregated\"\n","]\n","for notebook in notebooks:\n","    try:\n","        subprocess.run([\"databricks\", \"workspace\", \"import\", notebook], check=True)\n","        logging.info(f\"Successfully executed {notebook}\")\n","    except subprocess.CalledProcessError as e:\n","        logging.error(f\"Error occurred while executing {notebook}: {e}\")\n"],"metadata":{"id":"QfWWrBnBT7R9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 5"],"metadata":{"id":"NpDDFN-sUHZh"}},{"cell_type":"code","source":["\n","cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_cleaned\")\n","aggregated_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_aggregated\")\n","\n","total_transactions = cleaned_df.agg(sum(\"TransactionAmount\").alias(\"TotalTransactions\")).collect()[0][\"TotalTransactions\"]\n","\n","total_aggregated_transactions = aggregated_df.agg(sum(\"TotalTransactionAmount\").alias(\"TotalAggregatedTransactions\")).collect()[0][\"TotalAggregatedTransactions\"]\n","\n","if total_transactions == total_aggregated_transactions:\n","    print(f\"Data validation passed: {total_transactions} == {total_aggregated_transactions}\")\n","else:\n","    print(f\"Data validation failed: {total_transactions} != {total_aggregated_transactions}\")\n"],"metadata":{"id":"_nu1NrTNULcF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assignment 4"],"metadata":{"id":"-tDyau6IUY1W"}},{"cell_type":"markdown","source":["Task 1 Data Ingestion"],"metadata":{"id":"jl6Lq3W1UcYX"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import os\n","spark = SparkSession.builder.appName(\"Product Inventory Ingestion\").getOrCreate()\n","\n","file_path = \"/content/sample_data/tables/product_inventory.csv\"\n","logging.basicConfig(filename='/content/sample_data/logs/inventory_ingestion.log', level=logging.INFO)\n","\n","try:\n","    if os.path.exists(file_path):\n","        product_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n","        product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_raw\")\n","        logging.info(\"Product inventory ingestion completed successfully.\")\n","    else:\n","        raise FileNotFoundError(f\"File {file_path} not found.\")\n","\n","except FileNotFoundError as e:\n","    logging.error(f\"FileNotFoundError: {str(e)}\")\n","except Exception as e:\n","    logging.error(f\"An error occurred: {str(e)}\")\n"],"metadata":{"id":"XCtiB8MUWFTs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 2 Data Cleaning"],"metadata":{"id":"4-el3XQUWsrU"}},{"cell_type":"code","source":["\n","product_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/product_inventory_raw\")\n","cleaned_df = product_df.na.fill({\"StockQuantity\": 0, \"Price\": 0.0})\n","cleaned_df = cleaned_df.filter(col(\"StockQuantity\") >= 0)\n","\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_cleaned\")\n","print(\"Product inventory cleaning completed successfully.\")\n"],"metadata":{"id":"l0fXHEpGWx2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 3 Inventory Analysis"],"metadata":{"id":"Z6xvjVA3W7vo"}},{"cell_type":"code","source":["from pyspark.sql.functions import col, expr\n","cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/product_inventory_cleaned\")\n","\n","stock_value_df = cleaned_df.withColumn(\"TotalStockValue\", col(\"StockQuantity\") * col(\"Price\"))\n","restock_df = cleaned_df.filter(col(\"StockQuantity\") < 100)\n","\n","stock_value_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_analysis\")\n","restock_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_restock\")\n","print(\"Product inventory analysis completed successfully.\")\n"],"metadata":{"id":"twx6q4xCXBU1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 4 Build an Inventory Pipeline"],"metadata":{"id":"bAMUPUZMXMY-"}},{"cell_type":"code","source":["import subprocess\n","import logging\n","logging.basicConfig(filename='/content/sample_data/logs/inventory_pipeline_log.log', level=logging.INFO)\n","\n","notebooks = [\n","    \"/content/sample_data/delta/product_inventory_raw\",\n","    \"/content/sample_data/delta/product_inventory_cleaned\",\n","    \"/content/sample_data/delta/product_inventory_analysis\"\n","]\n","\n","for notebook in notebooks:\n","    try:\n","        subprocess.run([\"databricks\", \"workspace\", \"import\", notebook], check=True)\n","        logging.info(f\"Successfully executed {notebook}\")\n","    except subprocess.CalledProcessError as e:\n","        logging.error(f\"Error occurred while executing {notebook}: {e}\")\n"],"metadata":{"id":"Kdbp3qzyXRL0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 5"],"metadata":{"id":"pgTpGhdaXkam"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Inventory Monitoring\").getOrCreate()\n","\n","inventory_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/product_inventory_cleaned\")\n","\n","\n","urgent_restock_df = inventory_df.filter(col(\"StockQuantity\") < 50)\n","\n","if urgent_restock_df.count() > 0:\n","    print(\"Alert: Some products need urgent restocking!\")\n","    urgent_restock_df.show()\n","else:\n","    print(\"No products need urgent restocking at the moment.\")\n"],"metadata":{"id":"Lb_mqTLHXmFn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assignment 5"],"metadata":{"id":"lNWQh_KiXzsl"}},{"cell_type":"markdown","source":["Task 1 Data Ingession"],"metadata":{"id":"b8louozKX3Kt"}},{"cell_type":"code","source":["import logging\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"EmployeeAttendance\").getOrCreate()\n","logging.basicConfig(filename='/content/sample_data/logs/attendance_log.log', level=logging.INFO)\n","csv_file_path = \"/content/sample_data/employee_attendance.csv\"\n","\n","try:\n","    attendance_df = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n","    attendance_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/employee_attendance_raw\")\n","\n","    logging.info(\"Employee attendance data ingested successfully.\")\n","\n","except Exception as e:\n","    logging.error(f\"Error ingesting data: {str(e)}\")\n","    print(f\"Error: {str(e)}\")\n"],"metadata":{"id":"Gs3SC6ytX6WP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 2 Data Cleaning"],"metadata":{"id":"rWsj5ggDcpI-"}},{"cell_type":"code","source":["from pyspark.sql.functions import col, unix_timestamp, round\n","attendance_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/employee_attendance_raw\")\n","\n","cleaned_df = attendance_df.filter(col(\"CheckInTime\").isNotNull() & col(\"CheckOutTime\").isNotNull())\n","cleaned_df = cleaned_df.withColumn(\n","    \"HoursWorked\",\n","    round((unix_timestamp(col(\"CheckOutTime\"), \"HH:mm\") - unix_timestamp(col(\"CheckInTime\"), \"HH:mm\")) / 3600, 2)\n",")\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/employee_attendance_cleaned\")\n","\n","print(\"Employee attendance cleaning completed successfully.\")\n"],"metadata":{"id":"ISg6eAkscw7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 3  Attendance Summary"],"metadata":{"id":"6Aygs2msc6ZG"}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n","cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/employee_attendance_cleaned\")\n","\n","attendance_summary = cleaned_df.groupBy(\"EmployeeID\").agg(sum(\"HoursWorked\").alias(\"TotalHoursWorked\"))\n","overtime_df = cleaned_df.filter(col(\"HoursWorked\") > 8).select(\"EmployeeID\", \"Date\", \"HoursWorked\")\n","\n","attendance_summary.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/employee_attendance_summary\")\n","overtime_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/employee_overtime_summary\")\n","\n","logging.info(\"Employee attendance summary and overtime analysis completed.\")\n"],"metadata":{"id":"l6ITNGz2c54a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 4 Create an Attendance Pipeline"],"metadata":{"id":"yh-0h6U4eDNo"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, unix_timestamp, sum\n","\n","spark = SparkSession.builder.appName(\"EmployeeAttendancePipeline\").getOrCreate()\n","\n","def attendance_pipeline():\n","    try:\n","\n","        attendance_df = spark.read.option(\"header\", \"true\").csv(\"/content/sample_data/employee_attendance.csv\")\n","        attendance_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/attendance\")\n","        cleaned_df = attendance_df.filter(col(\"CheckInTime\").isNotNull() & col(\"CheckOutTime\").isNotNull())\n","\n","        cleaned_df = cleaned_df.withColumn(\n","            \"HoursWorked\",\n","            (unix_timestamp(col(\"CheckOutTime\"), 'HH:mm') - unix_timestamp(col(\"CheckInTime\"), 'HH:mm')) / 3600\n","        )\n","        cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/cleaned_attendance\")\n","        monthly_summary_df = cleaned_df.groupBy(\"EmployeeID\").agg(sum(\"HoursWorked\").alias(\"TotalHoursWorked\"))\n","        overtime_df = cleaned_df.filter(col(\"HoursWorked\") > 8)\n","\n","        monthly_summary_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/attendance_summary\")\n","        overtime_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/overtime_summary\")\n","\n","        print(\"Attendance pipeline completed successfully.\")\n","\n","    except FileNotFoundError:\n","        print(\"CSV file is missing.\")\n","    except Exception as e:\n","        print(f\"Error in pipeline: {e}\")\n","\n","attendance_pipeline()\n","\n"],"metadata":{"id":"IrIq8vzneIFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 5"],"metadata":{"id":"brIvJfn6ejr7"}},{"cell_type":"code","source":["\n","attendance_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/content/sample_data/delta/employee_attendance_cleaned\")\n","\n","spark.sql(\"DESCRIBE HISTORY '/content/sample_data/delta/employee_attendance_cleaned'\").show()\n"],"metadata":{"id":"_2SrNASQelUY"},"execution_count":null,"outputs":[]}]}